{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skeleton.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongs0104/UpliftModeling/blob/master/skeleton-day-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ehi1150I7fxN",
        "outputId": "ae53c36b-b286-496d-8128-3a921f2ffb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozB9PWshzfq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
        "fluff, id = link_hilstrom.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Hillstrom.csv')\n",
        "hillstrom_df = pd.read_csv('Hillstrom.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7tAS92JMPe9U",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "link_ = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Lalonde.csv')\n",
        "lalonde_df = pd.read_csv('Lalonde.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZoNrZI5P80wJ",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
        "    \"\"\"\n",
        "    Preprocessing the dataset\n",
        "     - Use one-hot encoding for categorical features\n",
        "     - Check the name of the target variable and treatment variable\n",
        "     - Drop the unused columns\n",
        "     - Delete the unused data\n",
        "    \n",
        "    Args:\n",
        "        df: A pandas.DataFrame which have all data of the dataset\n",
        "        dataset: the name of the dataset\n",
        "    Return:\n",
        "        # I recommend to split into the dataframes of predictor variables, the \n",
        "        # target variable, and the treatment varaible\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    if dataset in ['hillstrom', 'email']:\n",
        "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
        "    #   as the target variable of interest and the selected treatment is \n",
        "    #   the e-mail campaign for women’s merchandise [1]\n",
        "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
        "    #    J Market Anal. 2014;2:218–238\n",
        "        columns = df.columns\n",
        "        for col in columns:\n",
        "            if df[col].dtype != object:\n",
        "                continue\n",
        "            df = pd.concat(\n",
        "                  [df, pd.get_dummies(df[col], \n",
        "                                      prefix=col, \n",
        "                                      drop_first=False)],\n",
        "                  axis=1)\n",
        "            df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
        "                    for col in df.columns]\n",
        "        df = df[df.segment_mens_email == 0]\n",
        "        df.index = range(len(df))\n",
        "        # 대상자 e-mail campaign for women’s merchandise\n",
        "        # 불필요 정보 제거\n",
        "        df.drop(['segment_mens_email', \n",
        "               'segment_no_email', \n",
        "               'conversion', \n",
        "               'spend'], axis=1, inplace=True)\n",
        "\n",
        "        y_name = 'visit'\n",
        "        t_name = 'segment_womens_email'\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    \"\"\"  \n",
        "    elif dataset in ['criteo', 'ad']:\n",
        "    ###############################\n",
        "    ###     Do it yourself!     ###\n",
        "    ###############################\n",
        "    elif dataset in ['lalonde', 'job']:\n",
        "    ###############################\n",
        "    ###     Do it yourself!     ###\n",
        "    ###############################\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    df['Y'] = df[y_name]\n",
        "    df.drop([y_name], axis=1, inplace=True)\n",
        "    df['T'] = df[t_name]\n",
        "    df.drop([t_name], axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Igf3QLgdJ1cW",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def performance(pr_y1_ct1, pr_y1_ct0, y, ct, groups=10):\n",
        "    \"\"\"\n",
        "    1. Split the total customers into the given number of groups\n",
        "    2. Calculate the statistics of each segment\n",
        "    \n",
        "    Args:\n",
        "        pr_y1_ct1: the series (list) of the customer's expected return\n",
        "        pr_y1_ct0: the expected return when a customer is not treated\n",
        "        y: the observed return of customers\n",
        "        ct: whther each customer is treated or not\n",
        "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
        "    Return:\n",
        "        DataFrame:\n",
        "            columns:\n",
        "                'n_y1_ct1': the number of treated responders\n",
        "                'n_y1_ct0': the number of not treated responders\n",
        "                'r_y1_ct1': the average return of treated customers\n",
        "                'r_y1_ct0': the average return of not treated customers\n",
        "                'n_ct1': the number of treated customers\n",
        "                'n_ct0': the number of not treated customers\n",
        "                'uplift': the average uplift (the average treatment effect)\n",
        "            rows: the index of groups\n",
        "    \"\"\"\n",
        "  \n",
        "    ### check valid arguments\n",
        "    if groups not in [5, 10, 20]:\n",
        "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
        "  \n",
        "    ### check for NAs.\n",
        "    if pr_y1_ct1.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in pr_y1_ct1\")\n",
        "    if pr_y1_ct0.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in pr_y1_ct0\")\n",
        "    if y.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in y\")\n",
        "    if ct.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in ct\")\n",
        "   \n",
        "    ### check valid values for ct\n",
        "    if set(ct) != {0, 1}:\n",
        "        raise Exception(\"uplift: ct must be either 0 or 1\")\n",
        "\n",
        "    ### define dif_pred\n",
        "    dif_pred = pr_y1_ct1 - pr_y1_ct0\n",
        "  \n",
        "    ### Make index same\n",
        "    y.index = dif_pred.index\n",
        "    ct.index = dif_pred.index\n",
        "    \n",
        "    mm = pd.DataFrame({\n",
        "        'dif_pred': dif_pred,\n",
        "        'y': y,\n",
        "        't': ct,\n",
        "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
        "    })\n",
        "\n",
        "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
        "  \n",
        "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
        "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
        "    n_t1 = mm_groupby['t'].sum()\n",
        "    n_t0 = mm_groupby['t'].count() - n_t1\n",
        "  \n",
        "    df = pd.DataFrame({\n",
        "        'n_t1': n_t1,\n",
        "        'n_t0': n_t0,\n",
        "        'n_y1_t1': n_y1_t1,\n",
        "        'n_y1_t0': n_y1_t0,\n",
        "        'r_y1_t1': n_y1_t1 / n_t1,\n",
        "        'r_y1_t0': n_y1_t0 / n_t0,\n",
        "    })\n",
        "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
        "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
        "    df.index.name = 'groups'\n",
        "\n",
        "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
        "    df['uplift'] = round(df['uplift'], 6)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def qini(perf, plotit=True):\n",
        "    nrow = len(perf)\n",
        "    \"\"\"\n",
        "    Calculating the incremental gains (y-axis of Qini curve)\n",
        "     - First, the cumulitative sum of the treated and the control groups are\n",
        "      calculated with respect to the total population in each group at the\n",
        "      specified decile\n",
        "     - Afterwards we calculate the percentage of the total amount of people\n",
        "      (both treatment and control) are present in each decile\n",
        "    Args:\n",
        "        perf: A return of the performance function (above)\n",
        "        plotit: whether draw a plot or not\n",
        "    Return:\n",
        "        1. Qini value\n",
        "        2. return or save the plot if plotit is True\n",
        "    \"\"\"\n",
        "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
        "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
        "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
        "\n",
        "    ### Model Incremental gains\n",
        "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
        "    inc_gains = [0.0] + list(inc_gains)\n",
        "\n",
        "    ### Overall incremental gains\n",
        "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
        "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
        "\n",
        "    ### Random incremental gains\n",
        "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
        "\n",
        "    ### Compute area under the model incremental gains (uplift) curve\n",
        "    x = [0] + deciles\n",
        "    y = list(inc_gains)\n",
        "    auuc = 0\n",
        "    auuc_rand = 0\n",
        "\n",
        "    auuc_list = [auuc]\n",
        "    for i in range(1, len(x)):\n",
        "        #면적\n",
        "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
        "        auuc_list.append(auuc)\n",
        "\n",
        "    ### Compute area under the random incremental gains curve\n",
        "    y_rand = random_inc_gains\n",
        "\n",
        "    auuc_rand_list = [auuc_rand]\n",
        "    for i in range(1, len(x)):\n",
        "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
        "        auuc_rand_list.append(auuc_rand)\n",
        "\n",
        "    ### Compute the difference between the areas (Qini coefficient)\n",
        "    Qini = auuc - auuc_rand\n",
        "\n",
        "    ### Plot incremental gains curve\n",
        "    if plotit:\n",
        "        x_axis = x\n",
        "        plt.plot(x_axis, inc_gains)\n",
        "        plt.plot(x_axis, random_inc_gains)\n",
        "        plt.show()\n",
        "    \n",
        "    ### Qini 30%, Qini 10%\n",
        "    n_30p = int(nrow*3/10)\n",
        "    n_10p = int(nrow/10)\n",
        "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
        "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
        "\n",
        "    res = {\n",
        "        'qini': Qini,\n",
        "        'inc_gains': inc_gains,\n",
        "        'random_inc_gains': random_inc_gains,\n",
        "        'auuc_list': auuc_list,\n",
        "        'auuc_rand_list': auuc_rand_list,\n",
        "        'qini_30p': qini_30p,\n",
        "        'qini_10p': qini_10p,\n",
        "    }    \n",
        "\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nrxjl1v7J9Mm",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def parameter_tuning(fit_mdl, pred_mdl, data, search_space):\n",
        "    \"\"\"\n",
        "    Given a model, search all combination of parameter sets and find\n",
        "    the best parameter set\n",
        "    \n",
        "    Args:\n",
        "        fit_mdl: model function\n",
        "        pred_mdl: predict function of fit_mdl\n",
        "        data:\n",
        "            {\n",
        "                \"x_train\": predictor variables of training dataset,\n",
        "                \"y_train\": target variables of training dataset,\n",
        "                \"ct_train\": treatment variables of training dataset,\n",
        "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
        "                \"y_test\": target variables of test (usually, validation) dataset,\n",
        "                \"ct_test\": treatment variables of test (usually, validation) dataset,\n",
        "            }\n",
        "        search_space:\n",
        "            {\n",
        "                parameter_name: [search values]\n",
        "            }\n",
        "    Return:\n",
        "        The best parameter set\n",
        "    \"\"\"\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['y_train']\n",
        "    t_train = data['t_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "    t_test = data['t_test']\n",
        "    \n",
        "    max_q = -float('inf')\n",
        "    best_mdl = None\n",
        "\n",
        "    keys = search_space.keys()\n",
        "    n_space = [len(search_space[key]) for key in keys]\n",
        "    n_iter = np.prod(n_space)\n",
        "    \n",
        "    best_params = None\n",
        "    for i in range(n_iter):\n",
        "        params = {}\n",
        "        for idx, key in enumerate(keys):\n",
        "            params[key] = search_space[key][i % n_space[idx]]\n",
        "            i = int(i / n_space[idx])\n",
        "\n",
        "        mdl = fit_mdl(x_train, y_train, t_train, **params)\n",
        "        pred = pred_mdl(mdl, newdata=x_test, y=y_test, ct=t_test)\n",
        "        # print('    {}'.format(params))\n",
        "        try:\n",
        "            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "        q = qini(perf, plotit=False)['qini']\n",
        "        if q > max_q:\n",
        "            max_q = q\n",
        "            best_mdl = mdl\n",
        "            best_params = params\n",
        "\n",
        "    return best_mdl, best_params\n",
        "\n",
        "  \n",
        "def wrapper(fit_mdl, pred_mdl, data, params=None,\n",
        "            best_models=None, drop_variables=None, qini_values=None):\n",
        "    \"\"\"\n",
        "    General wrapper approach\n",
        "    \n",
        "    Args:\n",
        "        fit_mdl: model function\n",
        "        pred_mdl: predict function of fit_mdl\n",
        "        data:\n",
        "            {\n",
        "                \"x_train\": predictor variables of training dataset,\n",
        "                \"y_train\": target variables of training dataset,\n",
        "                \"ct_train\": treatment variables of training dataset,\n",
        "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
        "                \"y_test\": target variables of test (usually, validation) dataset,\n",
        "                \"ct_test\": treatment variables of test (usually, validation) dataset,\n",
        "            }\n",
        "    Return:\n",
        "        (A list of best models, The list of dropped variables)\n",
        "    \"\"\"\n",
        "    if best_models is None:\n",
        "        best_models = []\n",
        "    if drop_variables is None:\n",
        "        drop_variables = []\n",
        "    if qini_values is None:\n",
        "        qini_values = []\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['y_train']\n",
        "    t_train = data['t_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "    t_test = data['t_test']\n",
        "                        \n",
        "    variables = data['x_train'].columns\n",
        "\n",
        "    max_q = -float('inf')\n",
        "    drop_var = None\n",
        "    best_mdl = None\n",
        "    for var in variables:\n",
        "        if var in drop_variables:\n",
        "            continue\n",
        "        x = x_train.copy()\n",
        "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
        "        mdl = fit_mdl(x, y_train, t_train, **params)\n",
        "        x = x_test.copy()\n",
        "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
        "        pred = pred_mdl(mdl, newdata=x, y=y_test, ct=t_test)\n",
        "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
        "        q = qini(perf, plotit=False)['qini']\n",
        "        if q > max_q:\n",
        "            max_q = q\n",
        "            drop_var = var\n",
        "            best_mdl = mdl\n",
        "    \n",
        "    best_models.append(best_mdl)\n",
        "    drop_variables.append(drop_var)\n",
        "    qini_values.append(max_q)\n",
        "                        \n",
        "    left_vars = [var for var in variables if (var not in drop_variables)]\n",
        "\n",
        "    if len(variables) == len(drop_variables) + 1:\n",
        "        return best_models, drop_variables + left_vars, qini_values\n",
        "    else:\n",
        "        return wrapper(fit_mdl, pred_mdl, data, params=params,\n",
        "                       best_models=best_models, drop_variables=drop_variables,\n",
        "                       qini_values=qini_values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxrEbEODlbQi",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def tma(x, y, ct, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
        "    (a.k.a. \"Separate Model Approach\")\n",
        "    The default model is General Linear Model (GLM)\n",
        "    \n",
        "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        ct: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        Dictionary: A dictionary of two models. One for the treatment group, \n",
        "            one for the control group.\n",
        "\n",
        "            {\n",
        "                'model_treat': a model for the treatment group,\n",
        "                'model_control': a model for the control group\n",
        "            }\n",
        "\n",
        "    \"\"\"\n",
        "    treat_rows = (ct == 1)\n",
        "    control_rows = (ct == 0)\n",
        "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
        "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
        "    \n",
        "    return {'model_treat':model_treat, 'model_control':model_control}\n",
        "\n",
        "\n",
        "def predict_tma(obj, newdata, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
        "    (a.k.a. \"Separate Model Approach\")\n",
        "    \n",
        "    For each instance in newdata two predictions are made:\n",
        "    1) What is the probability of a person responding when treated?\n",
        "    2) What is the probability of a person responding when not treated\n",
        "      (i.e. part of control group)?\n",
        "\n",
        "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
        "\n",
        "    Args:\n",
        "        obj: A dictionary of two models. \n",
        "            One for the treatment group, one for the control group.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        DataFrame: A dataframe with predicted returns for when the customers\n",
        "            are treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "   \n",
        "    if isinstance(obj['model_treat'], LinearRegression):\n",
        "        pred_treat = obj['model_treat'].predict(newdata)\n",
        "    else:\n",
        "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
        "\n",
        "    if isinstance(obj['model_control'], LinearRegression):\n",
        "        pred_control = obj['model_control'].predict(newdata)\n",
        "    else:\n",
        "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
        "    \n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReoiyJFTMilO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import math\n",
        "\n",
        "class Node(object):\n",
        "    def __init__(self, attribute, threshold):\n",
        "        self.attr = attribute\n",
        "        self.thres = threshold\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.leaf = False\n",
        "        self.predict = None\n",
        "\n",
        "\n",
        "def select_threshold(df, attribute, predict_attr):\n",
        "    \"\"\"\n",
        "    Select the threshold of the attribute to split\n",
        "    The threshold chosen splits the test data such that information gain is maximized\n",
        "    \"\"\"\n",
        "    # Convert dataframe column to a list and round each value\n",
        "    values = df[attribute].tolist()\n",
        "    values = [float(x) for x in values]\n",
        "    # Remove duplicate values by converting the list to a set, then sort the set\n",
        "    values = set(values)\n",
        "    values = list(values)\n",
        "    values.sort()\n",
        "    max_ig = float(\"-inf\")\n",
        "    thres_val = 0\n",
        "    # try all threshold values that are half-way between successive values in this sorted list\n",
        "    for i in range(0, len(values) - 1):\n",
        "        thres = (values[i] + values[i+1])/2\n",
        "        ig = info_gain(df, attribute, predict_attr, thres)\n",
        "        if ig > max_ig:\n",
        "            max_ig = ig\n",
        "            thres_val = thres\n",
        "    # Return the threshold value that maximizes information gained\n",
        "    return thres_val\n",
        "\n",
        "\n",
        "def info_entropy(df, predict_attr):\n",
        "    \"\"\"\n",
        "    Calculate info content (entropy) of the test data\n",
        "    \"\"\"\n",
        "    # Dataframe and number of positive/negatives examples in the data\n",
        "    p_df = df[df[predict_attr] == 1]\n",
        "    n_df = df[df[predict_attr] == 0]\n",
        "    p = float(p_df.shape[0])\n",
        "    n = float(n_df.shape[0])\n",
        "    # Calculate entropy\n",
        "    if p  == 0 or n == 0:\n",
        "        I = 0\n",
        "    else:\n",
        "        I = ((-1*p)/(p + n))*math.log(p/(p+n), 2) + ((-1*n)/(p + n))*math.log(n/(p+n), 2)\n",
        "    return I\n",
        "\n",
        "\n",
        "def remainder(df, df_subsets, predict_attr):\n",
        "    \"\"\"\n",
        "    Calculates the weighted average of the entropy after an attribute test\n",
        "    \"\"\"\n",
        "    # number of test data\n",
        "    num_data = df.shape[0]\n",
        "    remainder = float(0)\n",
        "    for df_sub in df_subsets:\n",
        "        if df_sub.shape[0] > 1:\n",
        "            remainder += float(df_sub.shape[0]/num_data)*info_entropy(df_sub, predict_attr)\n",
        "    return remainder\n",
        "\n",
        "\n",
        "def info_gain(df, attribute, predict_attr, threshold):\n",
        "    \"\"\"\n",
        "    Calculates the information gain from the attribute test based on a given threshold\n",
        "    Note: thresholds can change for the same attribute over time\n",
        "    \"\"\"\n",
        "    sub_1 = df[df[attribute] <= threshold]\n",
        "    sub_2 = df[df[attribute] > threshold]\n",
        "    # Determine information content, and subract remainder of attributes from it\n",
        "    ig = info_entropy(df, predict_attr) - remainder(df, [sub_1, sub_2], predict_attr)\n",
        "    return ig\n",
        "\n",
        "\n",
        "def num_class(df, predict_attr):\n",
        "    \"\"\"\n",
        "    Returns the number of positive and negative data\n",
        "    \"\"\"\n",
        "    p_df = df[df[predict_attr] == 1]\n",
        "    n_df = df[df[predict_attr] == 0]\n",
        "    return p_df.shape[0], n_df.shape[0]\n",
        "\n",
        "\n",
        "def choose_attr(df, attributes, predict_attr):\n",
        "    \"\"\"\n",
        "    Chooses the attribute and its threshold with the highest info gain\n",
        "    from the set of attributes\n",
        "    \"\"\"\n",
        "    max_info_gain = float(\"-inf\")\n",
        "    best_attr = None\n",
        "    threshold = 0\n",
        "    # Test each attribute (note attributes maybe be chosen more than once)\n",
        "    for attr in attributes:\n",
        "        thres = select_threshold(df, attr, predict_attr)\n",
        "        ig = info_gain(df, attr, predict_attr, thres)\n",
        "        if ig > max_info_gain:\n",
        "            max_info_gain = ig\n",
        "            best_attr = attr\n",
        "            threshold = thres\n",
        "    return best_attr, threshold\n",
        "\n",
        "\n",
        "\n",
        "def build_tree(df, cols, predict_attr):\n",
        "    \"\"\"\n",
        "    Builds the Decision Tree based on training data, attributes to train on,\n",
        "    and a prediction attribute\n",
        "    \"\"\"\n",
        "    # Get the number of positive and negative examples in the training data\n",
        "    p, n = num_class(df, predict_attr)\n",
        "    # If train data has all positive or all negative values or less than the\n",
        "    # given minimum number of split. Then we have reached the end of our tree\n",
        "    if p == 0 or n == 0 or (p+n) < 100:\n",
        "        # Create a leaf node indicating it's prediction\n",
        "        leaf = Node(None,None)\n",
        "        leaf.leaf = True\n",
        "        leaf.predict = p / (p+n)\n",
        "        return leaf\n",
        "    else:\n",
        "        # Determine attribute and its threshold value with the highest\n",
        "        # information gain\n",
        "        best_attr, threshold = choose_attr(df, cols, predict_attr)\n",
        "        # Create internal tree node based on attribute and it's threshold\n",
        "        tree = Node(best_attr, threshold)\n",
        "        sub_1 = df[df[best_attr] <= threshold]\n",
        "        sub_2 = df[df[best_attr] > threshold]\n",
        "        # Recursively build left and right subtree\n",
        "        tree.left = build_tree(sub_1, cols, predict_attr)\n",
        "        tree.right = build_tree(sub_2, cols, predict_attr)\n",
        "        return tree\n",
        "\n",
        "\n",
        "def predict(node, row_df):\n",
        "    \"\"\"\n",
        "    Given a instance of a training data, make a prediction of an observation (row)\n",
        "    based on the Decision Tree\n",
        "    Assumes all data has been cleaned (i.e. no NULL data)\n",
        "    \"\"\"\n",
        "    # If we are at a leaf node, return the prediction of the leaf node\n",
        "    if node.leaf:\n",
        "        return node.predict\n",
        "    # Traverse left or right subtree based on instance's data\n",
        "    if row_df[node.attr] <= node.thres:\n",
        "        return predict(node.left, row_df)\n",
        "    elif row_df[node.attr] > node.thres:\n",
        "        return predict(node.right, row_df)\n",
        "\n",
        "\n",
        "def test_predictions(root, df, target_attr='Y'):\n",
        "    \"\"\"\n",
        "    Given a set of data, make a prediction for each instance using the Decision Tree\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "    for index,row in df.iterrows():\n",
        "        prediction.append(predict(root, row))\n",
        "    pred_df = pd.Series(prediction)\n",
        "    return pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w92aJLmDNEBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ty_assign(y, t):\n",
        "    if y == 1 and t == 1:\n",
        "        return \"TR\"\n",
        "    elif y == 0 and t == 1:\n",
        "        return \"TN\"\n",
        "    elif y == 1 and t == 0:\n",
        "        return \"CR\"\n",
        "    elif y == 0 and t == 0:\n",
        "        return \"CN\"\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "def ct_y_assign(y, ct):\n",
        "    if y == 1 and ct == 1:\n",
        "        return \"TR\"\n",
        "    elif y == 0 and ct == 1:\n",
        "        return \"TN\"\n",
        "    elif y == 1 and ct == 0:\n",
        "        return \"CR\"\n",
        "    elif y == 0 and ct == 0:\n",
        "        return \"CN\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def y_assign(ct_y):\n",
        "    if ct_y in (\"TR\", \"TN\"):\n",
        "        return 1\n",
        "    elif ct_y in (\"CR\", \"CN\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def ct_assign(ct_y):\n",
        "    if ct_y in (\"TR\", \"CR\"):\n",
        "        return 1\n",
        "    elif ct_y in (\"TN\", \"CN\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hREl_CRv9DYC",
        "outputId": "26f7e069-6bf8-49fa-de0b-3a0060fe6fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dataset='hillstrom'\n",
        "\n",
        "search_space = { \n",
        "    'method': [LogisticRegression],\n",
        "    'solver': ['newton-cg','libfgs','sag','saga'],\n",
        "    'pernalty':['none','l2'],\n",
        "    'tol': [1e-2, 1e-3, 1e-4],\n",
        "    'C': [1e6,1e3,1e-3,1e-6]\n",
        "}\n",
        "\n",
        "def main():\n",
        "    ### Load data ###\n",
        "    hdf = preprocess_data(hillstrom_df)\n",
        "    models = [LogisticRegression]\n",
        "    for model in models:\n",
        "        x = hdf.drop(columns=['Y','T'], axis=1)\n",
        "        Y = hdf['Y']\n",
        "        T = hdf['T']\n",
        "        ct_y = pd.DataFrame({'Y': Y, 'T': T})\\\n",
        "             .apply(lambda row: ct_y_assign(row['Y'], row['T']), axis=1)\n",
        "        fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(x, ct_y)\n",
        "        ### Cross validation ###\n",
        "        qini_list = []\n",
        "        for idx, (train_index, test_index) in enumerate(fold_gen):\n",
        "            x_train = x.reindex(train_index)\n",
        "            x_test = x.reindex(test_index)\n",
        "            y_train = Y.reindex(train_index)\n",
        "            y_test = Y.reindex(test_index)\n",
        "            ct_train = T.reindex(train_index)\n",
        "            ct_test = T.reindex(test_index)\n",
        "            \n",
        "            df = x_train.copy()\n",
        "            df['Y'] = y_train\n",
        "            df['T'] = ct_train\n",
        "            stratify = ct_train\n",
        "            if dataset == 'hillstrom':\n",
        "                stratify = df[['Y', 'T']]\n",
        "            tuning_df, validate_df = train_test_split(\n",
        "                df, test_size=0.33, random_state=1234, stratify=stratify)\n",
        "            x_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
        "            y_tuning = tuning_df['Y']\n",
        "            t_tuning = tuning_df['T']\n",
        "\n",
        "            x_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
        "            y_validate = validate_df['Y']\n",
        "            t_validate = validate_df['T']\n",
        "\n",
        "            data_dict = {\n",
        "                \"x_train\": x_tuning,\n",
        "                \"y_train\": y_tuning,\n",
        "                \"t_train\": t_tuning,\n",
        "                \"x_test\": x_validate,\n",
        "                \"y_test\": y_validate,\n",
        "                \"t_test\": t_validate,\n",
        "            }\n",
        "            model_method = search_space.get('method', None)\n",
        "            params = {\n",
        "                'method': None if model_method is None else model_method[0],\n",
        "            }\n",
        "            if params['method'] == LogisticRegression:\n",
        "                solver = search_space.get('solver', None)\n",
        "                params['solver'] = None if solver is None else solver[0]\n",
        "                \n",
        "            #_, drop_vars, qini_values = wrapper(\n",
        "            #    tma, predict_tma, data_dict, params=params)\n",
        "            #best_qini = max(qini_values)\n",
        "            #best_idx = qini_values.index(best_qini)\n",
        "            #best_drop_vars = drop_vars[:best_idx]\n",
        "            \n",
        "        ### Parameter tuning ###\n",
        "            #_, best_params = parameter_tuning(tma, predict_tma, data_dict, \n",
        "            #                              search_space=search_space)\n",
        "            \n",
        "            tm = tma(x_train, y_train, ct_train)\n",
        "            pred = predict_tma(obj=tm, newdata=x_test)\n",
        "            perf = performance(pred['pr_y1_t1'],pred['pr_y1_t0'],y_test,ct_test)\n",
        "            q = qini(perf)\n",
        "            print(perf)\n",
        "            qini_list.append(q['qini'])\n",
        "            print('Qini values: ', qini_list)\n",
        "            print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n",
        "            \n",
        "            df_train = x_train.copy()\n",
        "            df_train['Y'] = y_train\n",
        "            df_train['T'] = ct_train\n",
        "            df_test = x_test.copy()\n",
        "            df_test['Y'] = y_test\n",
        "            df_test['T'] = ct_test\n",
        "            df_train.drop(['history'], axis=1, inplace=True)\n",
        "            df_test.drop(['history'], axis=1, inplace=True)\n",
        "\n",
        "            assert((df_train.columns == df_test.columns).all())\n",
        "            attributes = [c for c in df_train.columns if c != 'Y']\n",
        "            \n",
        "            root = build_tree(df_train, attributes, 'Y')\n",
        "            pred = test_predictions(root, df_test, target_attr='Y')\n",
        "            #print('pred: {}'.format(pred))\n",
        "#         print(\"Model: {}\\n\".format(model))\n",
        "#         print(\"Tuning space: \\n\")\n",
        "#         for key, val in search_space.items():\n",
        "#             print(\"    '{}': {}\\n\".format(key, val))\n",
        "#         print(\"Seed: {}\\n\".format(seed))\n",
        "#         print(\"Qini value: mean = {}, std = {}\\n\\n\".format(mean_qini, std_qini))\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VNXWx/HvTm8kAUJNIfROKJGq\nCCiIKOAVFaTYuPpasPerF72Wq9gLVhQRUFCxEKUpAtJL6B0CSSABAum9TGa/f5zRGxHMAJOcKevz\nPHnIzJxk1knC75zZZ8/aSmuNEEIIz+BldgFCCCFqj4S+EEJ4EAl9IYTwIBL6QgjhQST0hRDCg0jo\nCyGEB5HQF0IIDyKhL4QQHkRCXwghPIiP2QWcLiIiQsfGxppdhhBCuJTNmzdnaq0bVLed04V+bGws\niYmJZpchhBAuRSmVas92MrwjhBAeREJfCCE8iIS+EEJ4EAl9IYTwIBL6QgjhQST0hRDCg0joCyGE\nB3G6efpCCOFJisst7E7Po2TjDALCG9PzinE1+nwS+kIIUUsqKq3sP1HA9rRcdhzNY3taLiUZSfzX\n5xP6e+9mc52BIKEvhHAnWmvKLFYKSi0UllkoLLVQUFZBoe327/f7+3gRWz+Y2IggousF4e/jbXbp\n58Rq1SRnFbEjLZfttoDfcyyfMosVgHqBXjwc9hs3BH6G8vImf8Cr9Oj7zxqvS0JfCGEXrTUlFZW2\nkLaFdamFwrKKPwV4Ydn/Hv9fqFsoKK3447bFqs/puZWCpmGBNI8Ipln9INvBIJjY+sYBIcDX3AOC\n1poT+aVsP5rL9rQ8dqTlsiMtj4JSCwCBvt50jgxjQu9mxEWHEx+YQePfHkGlJ0LrK+DqNwkNi6yV\nWiX0hRB/UVhm4dvNaXy3NZ3MgrI/AtuerPb38aJOgA91AnwJ8fchxN+HqLqB1PGvQ0iAcTvE9ngd\n///dDvH3oU6Vx0vKK0nJKiYls4iUrCLbv8Us2Hmc3OKKP57v9wNCbEQQzeobB4LfDwoxNXRAyC0u\nN8L9aC7b04ygP1VQBoCPl6J9k1BGxDUlLiqcLtFhtGoQgo+3F1jKYfWbMP9VCAiFUZ9Cp1HGTtQS\nCX0hxB9SMov4fF0K3ySmUVhmoXNkGL1b1P9TGFcN59+D/ffbwf4++Pk4ZlKgv483XYP86Bod/pfH\ncovLSc0qth0MbP9mFbFo53FyTjsgNAkNIDYi+H8HhIhgYusbrxjsOSAUl1vYfSz/T2fxqVnFf3z/\nFhHBXNIqgrjocLpEhdG+SeiZv2/6Zph/L5zcDZ2ugyunQHDE+f+AzpOEvhAeTmvNqoOZzFibwvL9\nJ/HxUlzVuQk3942lW0xds8s7o/AgP8KD/Ig7wwEhr7iC1OwikjOLjAOD7ZXCkt0nyC4q/9O2TcIC\n/rhuYBwUgokI8WN/RsEfF1oPZBT88QonMjyQLlFhjLkohrjoMDpHhlEnwPfviy0vhhX/hXXvQUhj\nuHEutL3SUT+KcyahL4SHKiqz8N2WNGasTeHQqSIiQvy5b1BrxvWKoWFogNnlnbewIF+6BIXTJeoM\nB4SSCo5kFZOcVURqZpHxb1YxP+/OIOu0A0LdIF+6RIUzpGNj4qLC6BIVToM6/udWTPIq+PE+yD4M\nPW6Fwf+BgLAL2b0LJqEvhIdJzSpi5rpUvt50lIIyC12iwnhzdBzDOjdxuRky5yos0JfOUWF0jvpr\n8OaXGgeEkwWltG5Yh6i6gajzHWsvzYNfnoHNn0Hd5nDzj9C8/wVW7xgS+kJ4AK01a5KymLE2mV/3\nncRbKYZ1bsIt/WLpFh1+/uHmRkIDfOkUGQZc4Jn4/sXw04NQeAL63gsD/gV+QQ6p0REk9IVwY0Vl\nFr7bms7na1NIOllI/WA/7h3YinG9m9HIhYdwnFJRJix6HHbNg4YdYPRsiOphdlV/IaEvhBs6klXM\nzHUpfJV4lIJSYxbO69fHcXWc+w/h1DqtYde3sOgxKM03zuwvfhB8/Myu7Iwk9IVwE1pr1h7K4rM1\nKfy6LwNvpbiycxNu6RtL9xgZwqkReemw4CE4sBgie8CIqdCog9lV/S0JfSFcXHG5he+3pjNjTQoH\nbUM4kwa2YlyvZjQOkyGcGmG1wpbP4ZfJUFkBV/wXet0JXs7/KkpCXwgXdTS7mFnrU5m78Qj5pRY6\nNg3ltevjuLpLE9PbEri1rEPw4/2QssqYkTP8HajX3Oyq7CahL4QL0Vqz7nAWM9aksHRvBkophnZq\nzK19Y+nRrK4M4dSkSgusfx+WvwjefkbYd7+pVlsoOIKEvhAuoKS8ku9ts3D2ZxRQL9iPuwe0Ylzv\nGJqEBZpdnvvL2A3zJ8GxLdB2GFz1OoQ2Nbuq8yKhL4QTS88tYea6FOZuPEpeSQUdmoTy6nVdGB7X\nVIZwaoOlDFa9bnwEhMN106HjtS53dl+VhL4QTkZrzZYjuUxfk8ziXSfQWhtDOP2aEy9DOLUnLdE4\nuz+1F7qMhiteguD6Zld1wST0hXAS5RYri3YdZ/rqZLan5REa4MM/L27OhD7NiKrrPO/odHvlRbDs\nRWP8PrQpjP0G2gwxuyqHsSv0lVJDgbcBb+ATrfXLpz3uD8wEegBZwGitdUqVx2OAPcCzWuvXHFO6\nEO4hu6icORuPMHNdChn5ZbSICOb5azoxqnskQX5yXlarDv9mNEjLSYH4iXD5s0bfezdS7V+UUsob\neA8YDKQBm5RSCVrrPVU2mwjkaK1bKaXGAFOA0VUefwNY5LiyhXB9BzIK+GxNMt9tSafMYuWS1hG8\nPKoLl7ZugJeXDOHUqpJc+OXfsGUm1GsBtyyE2H5mV1Uj7DmN6Akkaa0PAyil5gIjMc7cfzcSeNb2\n+TxgqlJKaa21UuoaIBkocljVQrgoq1Wz4sBJpq9OYXVSJv4+XlzbPYpb+8XSplEds8vzTPsWwE8P\nQdFJ6Hc/DHgSfN13RpQ9oR8JHK1yOw3odbZttNYWpVQeUF8pVQo8jvEq4ZELL1cI11RUZuHbLWl8\ntiaF5MwiGoX68+gVbRnbM4a6wc7Zo8XtFZ4y+uXs/g4adYIb50Bkd7OrqnE1PWD4LPCm1rrw72Yc\nKKXuAO4AiImJqeGShKg9aTnFzFyXypyNRygotRAXHc7bY7oyrHMTfL0ds6ygOEdaw46vYfHjxkXb\nQU9DvwfAu5oVsNyEPaGfDkRXuR1lu+9M26QppXwwGlJnYbwiuE4p9QoQDliVUqVa66lVv1hr/THw\nMUB8fLwdSy8L4by01iSm5vCZbcqlUoorOzXmtoub091Jlx/0GLlHjV73Sb9AVE8YORUatDW7qlpl\nT+hvAlorpZpjhPsYYOxp2yQANwPrgOuAZVprDVzy+wZKqWeBwtMDXwh3UW6xsmDnMaavTmFneh5h\ngb7c0b8lN/VpRtNw9x0jdglWK2yebqxmpa0wdAr0vN0lGqQ5WrWhbxujnwQswZiyOV1rvVsp9RyQ\nqLVOAD4FZimlkoBsjAODEB4hq7CMLzccYeb6VE4VlNGyQTAvXNOJa2XKpXPITIKEe+HIWmgxAIa/\nDXVjTS7KPMo4IXce8fHxOjEx0ewyhKjW3uP5fLYmmR+2HaPcYqV/mwbc1i+W/jLl0jlUWmDdVFjx\nEvj4G+2Pu45z6RYKf0cptVlrHV/ddnIaIsQ5sFo1y/adZPqaZNYeyiLA14vrexhTLls1lCmXTuPE\nTph/DxzfDu2uNhqk1WlsdlVOQUJfCDvN3XiED387REpWMU3CAnh8aDtu7BlNeJBMuXQaFaWw8lVY\n8xYE1oMbZkKHkWZX5VQk9IWww9yNR3jiu53ERYfz7pC2DO3UWKZcOpsjGyBhEmQegLixcMWLEFTP\n7KqcjoS+ENXYkZbL5ITdXNwqgs9v64m3jNc7l7JCWPY8bPgIwqJg/LfQ6nKzq3JaEvpC/I2conLu\nmr2FBiH+vHNjNwl8Z5P0K/z4AOQdNaZgXjYZ/OXayt+R0BfiLCqtmvu/2sapgjK+ubMP9aRdgvMo\nyYElT8G2L6B+a7h1ETTrY3ZVLkFCX4izeHvpAVYeOMV//9GZuOhws8sRv9uTAAsfgaJMuPghuPRx\n8A0wuyqXIaEvxBn8ujeDd5YlcX2PKG7sGV39F4iaV5BhhP3eBGjcGcZ9A03izK7K5UjoC3Ga1Kwi\nHvxqGx2bhvL8NZ1keUKzaQ3b58DiJ6GiBC57Bvre6zEN0hxNQl+IKkrKK7lz9haUUnw4vocsPm62\nnFT46QE4tAxi+sCIdyGitdlVuTQJfSFstNY89f1O9p3IZ/otFxFdT9alNY3VCpumwdL/GG0Thr1m\nLF/oJe+NuFAS+kLYzN5whO+2pvPA5a0Z2Lah2eV4rlMHjAZpR9dDy8tg+FsQLutsOIqEvhDA1iM5\nPPfjbga2bcB9g2T4wBSVFbDmbfhtCvgGwTUfQNyNbtsgzSwS+sLjZRaWcfcXW2gUGsCbo7tKh0wz\nHN9uNEg7sRM6XAPDXoUQebVVEyT0hUezVFq5b85WsovK+fauvtI8rbZVlBhn9mvegeAIGD0b2g83\nuyq3JqEvPNrrvxxg7aEsXr2uC50iw8wux7OkrjMapGUlQbfxMOQFCJTlJGuahL7wWIt3neCDFYcY\n2yuG6+PlDVi1pqzAmJWzaZpxgXbCD9ByoNlVeQwJfeGRDp8q5JFvthMXFcYzwzuYXY7nOLjUmHef\nlwa97oJBT4N/iNlVeRQJfeFxisst3Dl7M77eivfH98DfR96AVeOKs2HJv4x31ka0hYk/Q3RPs6vy\nSBL6wqNorXni250knSxk5m29iAwPNLsk96Y17Jlv9MwpyYH+jxofPv5mV+axJPSFR5mxNoWE7cd4\n9Iq2XNw6wuxy3FvBCVjwMOz7CZp0hQnfG43ShKkk9IXHSEzJ5sUFe7m8fSPuurSl2eW4L61h62z4\n+SmwlMHl/4E+k8Bb4sYZyG9BeISTBaXc/cUWouoG8voNcfIGrJqSkwI/3g+HV0BMX1uDtFZmVyWq\nkNAXbq+i0sqkL7eSX1rBzIk9CQuUlrwOZ62EjR/Dr8+B8oKrXocet0mDNCckoS/c3pRF+9iYnM1b\no7vSrnGo2eW4n5P7jAZpaRuh1WC4+k0Il/c9OCsJfeHWFuw4zierk7m5TzOu6RZpdjnupbICVr8F\nK18BvxC4dhp0vl4apDk5CX3htpJOFvDovO10jwnnqavkDVgOdWwrzJ8EGbug47Vw5SsQ0sDsqoQd\nJPSFWyoss/B/szYT5OfN++N64OcjY8sOUVECK16Cte9CcEMY8yW0u8rsqsQ5kNAXbkdrzWPztpOS\nVczsib1oHBZgdknuIWU1JNwH2Yeg+00w+HkIDDe7KnGOJPSF2/lkVTILd57gX8Pa0adlfbPLcX2l\n+bD0GUicDuHN4Kb50GKA2VWJ8yShL9zKukNZvLx4H1d2asztl7QwuxzXd+Bno0Fa/jHofQ8Megr8\ngs2uSlwACX3hNk7klXLvnC00qx/Eq9fHoWQWyfkryoLFT8DOr6FBO5j4C0RfZHZVwgEk9IVbKLdY\nuefLLRSXVzLn9t6E+Muf9nnRGnZ/Bwsfg9JcuPRxuORhaZDmRuR/hnAL/124l82pOUwd243WjeqY\nXY5ryj8OCx6C/QuhaTcYMR8adzK7KuFgEvrC5f2wNZ0Za1P458XNubpLU7PLcT1aw5aZ8PO/obLM\nWLaw113SIM1N2TV5WSk1VCm1XymVpJR64gyP+yulvrI9vkEpFWu7v6dSapvtY7tS6h+OLV94un0n\n8nnyu530jK3H41e2M7sc15N9GD4fDj/eZ7Q9vmst9L1XAt+NVfubVUp5A+8Bg4E0YJNSKkFrvafK\nZhOBHK11K6XUGGAKMBrYBcRrrS1KqSbAdqXUj1pri8P3RHic/NIK7py1mToBPkwd1w1fb3kDlt2s\nlbD+A1j2Anj5wNVvQfebpUGaB7DncN4TSNJaHwZQSs0FRgJVQ38k8Kzt83nAVKWU0loXV9kmANAX\nXLEQgNWqefjr7aTllDDnjt40rCNvwLJbxh5ImATpm6HNULjqDQiTvkSewp7QjwSOVrmdBvQ62za2\ns/o8oD6QqZTqBUwHmgET5CxfOMKHKw/xy54MnhnegYti65ldjmuwlMPqN2DlaxAQCqM+hU6jpEGa\nh6nxgTut9Qago1KqPfC5UmqR1rq06jZKqTuAOwBiYmJquiTh4lYfzOS1JfsZEdeUW/rGml2Oa0jf\nbDRIO7nH6IQ59GUIluUiPZE9A3jpQNXm2FG2+864jVLKBwgDsqpuoLXeCxQCf5kDprX+WGsdr7WO\nb9BAOvWJszuaXcx9c7fSqmEIL13bWd6AVZ3yYljyFHxyOZTkwo1zYdQnEvgezJ4z/U1Aa6VUc4xw\nHwOMPW2bBOBmYB1wHbBMa61tX3PUNuTTDGgHpDiqeOFZlu7J4JF526m0aj4c34NgeQPW30teaSxu\nkpMCPW6Fwf+BgDCzqxImq/Z/jS2wJwFLAG9gutZ6t1LqOSBRa50AfArMUkolAdkYBwaAi4EnlFIV\ngBW4W2udWRM7ItxXucXKlMX7+HR1Mh2bhjJ1bHeaR0j/l7MqzYNfJsPmGVC3Odz8EzS/xOyqhJNQ\nWjvXhJr4+HidmJhodhnCSRzJKubeOVvYnpbHLX1jeXJYO/x9vM0uy3ntXwQ/PQiFGdDnHhjwL/AL\nMrsqUQuUUpu11vHVbSevj4XTWrjzOI/P24FS8OH47gzt1MTskpxXUSYsehx2zYOGHWHMFxDZw+yq\nhBOS0BdOp7SikhcW7GH2+iN0jQ7n3Ru7EV1PzlbPSGvYOQ8WPQZlBcaZ/cUPgo+f2ZUJJyWhL5zK\n4VOF3PPlVvYez+eO/i149Iq28k7bs8lLNxqkHVgMkfEwcio0bG92VcLJSegLp/HD1nT+9f1O/H28\nmH5LPIPaNTK7JOdktcKWGfDzZNCVcMVL0Ov/wEuudYjqSegL05WUV/JMwi6+TkyjZ2w93r6xK03C\nAs0uyzllHTLWqU1dDc0vheFvQ73mZlclXIiEvjDVgYwC7vliC0mnCrl3UCvuv6w1PjKc81eVFlj/\nPix/Ebz9YcS70G2CtFAQ50xCX5hCa803iWlMTthFiL8PM2/rySWt5d3YZ5SxG+bfA8e2Qtur4KrX\nIVRmMonzI6Eval1hmYWnv9/JD9uO0a9Vfd4c3VW6ZJ6JpQxWvW58BITDdZ9Bx3/I2b24IBL6olbt\nPpbHpC+3kppVxMOD23D3wFZ4e0mI/cXRTUb741P7oMsYGPoSBEk3UXHhJPRFrdBaM3t9Ks8v2Evd\nIF/m3N6bXi3qm12W8ykvMhY2Wf8BhEbCuHnQerDZVQk3IqEvalxeSQVPfreDhTtPMKBtA16/Po76\nIf5ml+V8Dq8wZubkpsJF/4TLnjH63gvhQBL6okZtP5rLpDlbOJ5bypNXtuP2S1rgJcM5f1aSCz8/\nDVtnQb2WcMtCiO1ndlXCTUnoixqhtebT1clMWbyPhnUC+Or/+tCjWV2zy3I+e3+CBQ9D0Sno9wAM\neAJ85T0KouZI6AuHyykq59F521m69yRDOjTi1eviCAvyNbss51J4EhY+Cnt+gEadYexcaNrN7KqE\nB5DQFw6VmJLNvXO2klVYzrPDO3Bz31hZ3aoqrWHHV7D4CeOi7aCnjTN8bzkoitohoS8cwmrVfPDb\nId745QBRdQP59q6+dI6SVZr+JPeo0es+6ReI6mk0SGvQ1uyqhIeR0BcXLLOwjAe/2saqg5lc3aUJ\nL13bmToBcub6B6sVEj+Fpc8aZ/pXvmLMzpEGacIEEvrigqw9lMn9c7eRX1LBS9d2ZsxF0TKcU1Xm\nQWMa5pG10GKg0SCtbjOzqxIeTEJfnJdKq+adXw/yzrKDtIgIZtbEnrRrLHPK/1BpgbXvwIqXwTcA\nRr4PXcdKCwVhOgl9cc4KyyzcOWszq5MyGdU9iudGdiTYX/6U/nB8h9FC4fh2aHe10SCtTmOzqxIC\nkNAX5yivpIJbPtvIjrQ8XhnVhRsuija7JOdRUQorX4HVb0FQfbhhJnQYaXZVQvyJhL6wW3ZRORM+\n3cDBjELeH9edKzrK2esfjqyHhHsh8wDEjYUrXpQGacIpSegLu5zML2XcJxs4kl3MtJvjubSN9L4H\noKwQfn0ONn4MYVEw/ltodbnZVQlxVhL6olrpuSWMm7aekwVlzLi1J31aSndMAJJ+hR8fgLyj0PN2\nuGwy+Ncxuyoh/paEvvhbqVlFjJ22gfzSCmb/sxfdY6R/DsXZRoO0bV9A/dZw6yJo1sfsqoSwi4S+\nOKukkwWMnbaBikorc27vTadIeYcte+bDgkegOAsufggufdyYkimEi5DQF2e051g+Ez7dgJeX4qv/\n60ObRh4+bFGQAQsfgb0J0LgzjJ8HTeLMrkqIcyahL/5i29Fcbvp0AyH+Pnxxe2+aRwSbXZJ5tIZt\nX8KSJ40pmZdNhr73SYM04bIk9MWfbEzO5rYZm6gX7MeXt/ciqm6Q2SWZJycVfrwfDi+H6N4w4l1o\n0MbsqoS4IBL64g+rDp7i9pmJRIYH8sU/e9M4zEPHqq1W2DQNlv7HaJsw7DWInwheXmZXJsQFk9AX\nACzdk8HdX2yhZcMQZk3sSYSnrmF7ar/xJqujG6DlZTD8LQiPMbsqIRxGQl/w045jPDB3Gx2bhvL5\nbT0JD/Izu6TaV1kBa96G36aAbxBc8yHEjZEGacLtSOh7uHmb03hs3nbim9Xj01viPbMP/rFtRoO0\nEzuhwzUw7FUIaWh2VULUCAl9DzZ7fSpP/7CLS1pH8NGEHgT5edifQ0WJcWa/5h0IjoDRs6H9cLOr\nEqJGedj/cvG7T1Yd5oUFe7m8fUOmju1OgK+HreKUutYYu89Kgm7jYcgLECjvNhbuz67pCEqpoUqp\n/UqpJKXUE2d43F8p9ZXt8Q1KqVjb/YOVUpuVUjtt/w5ybPniXGmteffXg7ywYC9XdW7CB+N7eFbg\nlxXAgofhsyuhshwm/AAj35PAFx6j2jN9pZQ38B4wGEgDNimlErTWe6psNhHI0Vq3UkqNAaYAo4FM\nYLjW+phSqhOwBIh09E4I+2iteWXJfj5YcYhru0fyyqgu+Hh70DTEg78YDdLy06HXXTDoafAPMbsq\nIWqVPcM7PYEkrfVhAKXUXGAkUDX0RwLP2j6fB0xVSimt9dYq2+wGApVS/lrrsguuXJwTq1Xz3E97\nmLE2hXG9Ynh+ZCe8vDxkZkpxNix+EnbMhYi2MPFniO5pdlVCmMKe0I8Ejla5nQb0Ots2WmuLUioP\nqI9xpv+7UcCWMwW+UuoO4A6AmBiZE+1olVbNU9/vZO6mo/zz4uY8dVV7z1i8XGvY8wMsfBRKcqD/\nY9D/EfDx0PcgCEEtXchVSnXEGPIZcqbHtdYfAx8DxMfH69qoyVNYKq08/M125m87xn2DWvHg4Dae\nEfgFJ4yx+30/QZOuMOF7o1GaEB7OntBPB6ouhBplu+9M26QppXyAMCALQCkVBXwP3KS1PnTBFQu7\nlVus3DdnK4t3n+CxoW25e0Ars0uqeVrD1tmw5CmoLIPBz0Hve8BbJqoJAfaF/iagtVKqOUa4jwHG\nnrZNAnAzsA64DlimtdZKqXBgAfCE1nqN48oW1SmtqOTO2ZtZsf8UzwzvwK39mptdUs3LTjYapCX/\nBs36GQ3S6rc0uyohnEq1oW8bo5+EMfPGG5iutd6tlHoOSNRaJwCfArOUUklANsaBAWAS0AqYrJSa\nbLtviNb6pKN3RPxPUZmFf36eyPrkLF6+tjNjerr5dRJrJWz4CJY9D8obrnoDetwqDdKEOAOltXMN\nocfHx+vExESzy3BZ+aUV3PrZJrYdzeX16+O4ppubz5A9uc9ooZC2CVoPgavfNBYoF8LDKKU2a63j\nq9tOBjrdSE5ROTdN38i+E/m8N7YbQzs1MbukmmMphzVvwcpXwS8Erp0Gna+XBmlCVENC302cLChl\nwicbSckq4uMJ8Qxs58YNw9K3GC0UMnZBp1EwdAqENDC7KiFcgoS+GzieV8K4aRs4kV/KZ7dcRN9W\nEWaXVDPKi2HFS7BuKoQ0gjFzoN0ws6sSwqVI6Lu4I1nFjP1kPXnFFcya2JMezeqZXVLNSFltnN1n\nH4buN8Hg5yEw3OyqhHA5EvouLD23hBs+WkeppZIvb+9N56gws0tyvNJ8WPoMJE6HurFwUwK0uNTs\nqoRwWRL6LuydpQfJLi5n/j39aN8k1OxyHO/AEvjpQSg4Dn0mwcCnwM+DF2oXwgEk9F3UsdwSvtua\nxo09Y9wv8IuyYPETsPNraNAebpgJUdXORBNC2EFC30VNW3UYreGO/i3MLsVxtIZd38Kix4xhnUuf\ngEseBh8PXLNXiBoioe+CMgvLmLPxCCO7RhJV102GO/KPGQ3S9i+Ept1h5FRo1NHsqoRwOxL6Luiz\nNcmUWazcNcAN+spoDVs+h5//DZUVMORF6H0XeHnQal5C1CIJfReTX1rBzLWpXNmpMa0auviqT9mH\nIeE+SFkFsZfAiHegnhsNVwnhhCT0XcysdakUlFlcu02ytRLWfwDLXgBvXxj+NnS/WVooCFELJPRd\nSEl5JdNXJ3NpmwZ0inTROfkZe4wGaemboc2VcPUbENrU7KqE8BgS+i5k7qYjZBWVM2mQC57lW8ph\n9Ruw8jUICIVRnxp9c+TsXohaJaHvIsotVj5eeZiesfW4KNbFWi2kbTbO7k/ugc43wNCXIbi+2VUJ\n4ZEk9F3ED1vTOZ5XykvXutA6r+XFsPxFWP8+1GkCY7+GNleYXZUQHk1C3wVUWjUf/HaITpGhXNrG\nRVoIJ680GqTlpED8bXD5f4xhHSGEqST0XcDCncdJzizig3HdUc4+Bl6aZ8y53/K5Mf3ylgUQe7HZ\nVQkhbCT0nZzWmveWJ9GyQTBXdGxsdjl/b/8io0FaYQb0vQ8GPCkN0oRwMhL6Tm75/pPsO1HAa9fH\n4eXlpGf5RZlGv5xd30LDjjDmS4jsbnZVQogzkNB3Ylprpi5LIjI8kJFdnXAuu9awc54R+OWFMPBp\n6He/NEgTwolJ6Dux9Yez2XIkl+dHdsTX28vscv4sLw1+eggOLoGoi2DEVGjYzuyqhBDVkNB3Yu+v\nSCIixJ/r46PNLuV/rFbY/BnI+qb3AAAPxElEQVT88gzoSmPOfc87pEGaEC5CQt9JbT+ay6qDmTxx\nZTsCfJ0kULMOGQ3SUldD80uNnjn1mptdlRDiHEjoO6n3VyQRGuDDuF4xZpcClRZY/x4s/y94+xtD\nOd3GSwsFIVyQhL4TOpBRwJLdGdx3WWvqBPiaW8yJXUYLhWNboe1VcNXrENrE3JqEEOdNQt8JfbDi\nEEF+3tzaN9a8IixlRnO01W9AYF24fgZ0uEbO7oVwcRL6TuZIVjEJ249xa99Y6gabNPXx6EaYPwky\n90PcjXDFfyHIxZq8CSHOSELfyXy08hDeSnG7GQuelxfBr8/Dhg8hNBLGzYPWg2u/DiFEjZHQdyIZ\n+aV8k5jGdfFRNAoNqN0nP7QcfrwPco/ARbfD5c+Af53arUEIUeMk9J3IJ6sOY7FaubN/LS54XpIL\nPz8FW2dDvZZw6yJo1rf2nl8IUask9J1ETlE5X2w4woi4psTUr6UmZXt/ggUPQ9EpuPhBuPRx8A2s\nnecWQphCQt9JzFibQnF5JXcPrIWlEAtPwsJHYc8P0KgzjJ0LTbvV/PMKIUwnoe8ECssszFibwpAO\njWjTqAbH0bWGHV/B4ieMi7aD/m00SPM2+b0AQohaI6HvBL5Yn0peSUXNnuXnHoWfHoCkpRDdy3hX\nbYM2Nfd8QginZFfrRqXUUKXUfqVUklLqiTM87q+U+sr2+AalVKzt/vpKqeVKqUKl1FTHlu4eSisq\nmbYqmYtbRdA1OtzxT2C1wsZp8H5vSF0HV74Cty6WwBfCQ1V7pq+U8gbeAwYDacAmpVSC1npPlc0m\nAjla61ZKqTHAFGA0UAr8G+hk+xCn+WZzGpmFZdwzsAbG1DMPGuvUHlkHLQYaDdLqNnP88wghXIY9\nZ/o9gSSt9WGtdTkwFxh52jYjgc9tn88DLlNKKa11kdZ6NUb4i9NUVFr56LdDdI8Jp3cLB77jtdIC\nq96AD/rByT0w8n2Y8L0EvhDCrjH9SOBoldtpQK+zbaO1tiil8oD6QKY9RSil7gDuAIiJcYKukrUk\nYdsx0nJK+M+Ijo5b8Pz4DqNB2vHt0H44DHsd6jRyzPcWQrg8p7iQq7X+GPgYID4+XptcTq2wWjXv\nr0iiXeM6DGrX8MK/YUUprHwFVr8FQfXhhpnQ4fQXZEIIT2dP6KcDVZduirLdd6Zt0pRSPkAYkOWQ\nCt3Uz3tOcOhUEe/e2O3Cz/KPbDDO7jMPQNdxMOQFaZAmhDgje0J/E9BaKdUcI9zHAGNP2yYBuBlY\nB1wHLNNae8QZ+/nQWvPe8kPE1g9iWOcL6E1fVgi/PgcbP4awaBj/HbS6zHGFCiHcTrWhbxujnwQs\nAbyB6Vrr3Uqp54BErXUC8CkwSymVBGRjHBgAUEqlAKGAn1LqGmDIaTN/PM7Kg5nsTM9jyqjOeHud\n51l+0q/w4wOQd9RYo/ayyeAf4thChRBux64xfa31QmDhafdNrvJ5KXD9Wb429gLqc0vvLU+iSVgA\n/+gWde5fXJwNPz8N276A+q3htsUQ09vxRQoh3JJTXMj1JJtSstmYnM0zwzvg52PXe+P+Z898WPAI\nFGfBJQ9D/8fAt5ZbMAshXJqEfi17f3kS9YL9GHPROUxNLciAhY/A3gRo3AXGfwtNutRckUIItyWh\nX4t2peexfP8pHr2iLYF+3tV/gdaw7UtY8i+oKIHLn4U+k6RBmhDivEno16IPVhyijr8P43vb8c7Y\nnFT48X44vBxi+sCIdyGidc0XKYRwaxL6teTQqUIW7jrO3QNaEhb4N2fqVitsmgZL/wNKwbDXIH4i\neJ3j+L8QQpyBhH4t+XDFIfx9vLi1X/Ozb3Rqv9Eg7egGaHU5XP0mhHtOWwohRM2T0K8FaTnFfL81\nnfG9mxER4v/XDSorYM3b8NsU8AuGf3wEXUYbZ/pCCOFAEvq1YNrKwygFd/Rv8dcHj22D+ZMgYyd0\nuAaGvQohDujFI4QQZyChX8NOFZQxd9NRru0WRdPwKouOV5TAipdh7bsQHAGjZxtdMYUQogZJ6New\n6WuSqai0cueAlv+7M3WtMXaflQTdJsCQ5yGwrnlFCiE8hoR+DcorqWDWulSGdW5C84hgKCuApc/C\npk+MC7QTfoCWA80uUwjhQST0a9DMtSkUllm4e0ArOPiL0SAtPx163w2DnjYu2gohRC2S0K8hxeUW\npq9JZkRrfzqsfxR2zIWItjDxZ4juaXZ5QggPJaFfQ+ZsOELf0lW8fuoLSM8zmqP1fwR8zjBlUwgh\naomEfg0oy0mj5bL/Y6LfRqjbFUYmQONOZpclhBAS+g6lNWydhVr4L3pbSznc/XFaXP0YeMuPWQjh\nHCSNHCU72WiQlvwbe7w68mG9+/lgxA3yrlohhFOR0L9Q1krY8BEsex6UN9vjJvOPDW34cNRFF77g\nuRBCOJiE/oU4uddooZCeCK2HYB32Bo/NSKZVQ83g9o3Mrk4IIf5C+vWeD0s5/PYKfHgJZB+Ga6fB\n2K/59bgf+zMKuHtgS7zOd8FzIYSoQXKmf67SN8P8e+Hkbug0CoZOgZAGaK2ZujyJ6HqBDO/S1Owq\nhRDijCT07VVeDCv+C+veg5BGMGYOtBv2x8PrDmWx/WguL/6jEz7e8gJKCOGcJPTtkbwKfrzPGMrp\nfrPRIC0g7E+bvLciiYZ1/BnVPcqkIoUQonoS+n+nNA9+eQY2fwZ1Y+GmBGhx6V8223okhzVJWTw1\nrD0BvnYseC6EECaR0D+bA0uMBmmFJ6DPJBj4FPgF/WmTikori3ad4J1fDxIe5MvYXrK0oRDCuUno\nn64oExY/ATu/gQbtYfQsiIr/0yaZhWXM2XCE2RtSycgvI7Z+EK9dF0ewv/w4hRDOTVLqd1rDrm9h\n0WNQmg8DnoSLHwIfvz822ZWex2drUvhx+zHKK630b9OAl6+N5dI2DWSKphDCJUjoA+Slw4KH4MBi\niOwBI6ZCow6AMYSzZPcJZqxJITE1hyA/b8b0jOamPrG0ahhicuFCCHFuPDv0rVbY8jn8MhkqK2DI\ni9D7LvDyJqvQWNt21rpUTuSXElMviH9f3YHr46MIDfA1u3IhhDgvnhv6WYeMBmkpqyD2EhjxDtRr\nwa70PD5fm8L87ccot1i5pHUEL/6jEwPaNsRbhnCEEC7O80LfWgnr34dlL4K3Lwx/G0vcBH7ee5IZ\nX69jY0o2gb7e3BAfxc19YmndqI7ZFQshhMN4Vuhn7IH598CxLdDmSnIGTWHOvgpmvbqC43mlRNcL\n5Omr2nN9fDRhgTKEI4RwP54R+pYyWPUGrHodAsJIu+w93j3RmR+m7qHMYqVfq/o8N7ITg9rJEI4Q\nwr25f+inJRrtj0/t5VjMcCaXjWfpgkoCfI8xqkcUt/SNpY0M4QghPIRdoa+UGgq8DXgDn2itXz7t\ncX9gJtADyAJGa61TbI89CUwEKoH7tNZLHFb93ykvgmUvote/T5F/Q571fYp5BzoSGe7Hv4Y1Y3R8\nDGFBMoQjhPAs1Ya+UsobeA8YDKQBm5RSCVrrPVU2mwjkaK1bKaXGAFOA0UqpDsAYoCPQFFiqlGqj\nta509I78yeHfKP9hEn75R5hrvZwX88bQuUU0H10Ty+XtG8kQjhDCY9lzpt8TSNJaHwZQSs0FRgJV\nQ38k8Kzt83nAVGWsFTgSmKu1LgOSlVJJtu+3zjHl/1llcQ7Hv3mUqORvSLc2YrKeTFS3wczrG0u7\nxqE18ZRCCOFS7An9SOBoldtpQK+zbaO1tiil8oD6tvvXn/a1kedd7d84sGUldRNuponO4Qufayjp\n/yjv9GpD3WC/6r9YCCE8hFNcyFVK3QHcARATc36dKhs3a0eqfywH+33E6H6Xy0ImQghxBvaEfjoQ\nXeV2lO2+M22TppTyAcIwLuja87VorT8GPgaIj4/X9hZfVWj9hnR+cvn5fKkQQngMe06HNwGtlVLN\nlVJ+GBdmE07bJgG42fb5dcAyrbW23T9GKeWvlGoOtAY2OqZ0IYQQ56raM33bGP0kYAnGlM3pWuvd\nSqnngEStdQLwKTDLdqE2G+PAgG27rzEu+lqAe2p85o4QQoizUsYJufOIj4/XiYmJZpchhBAuRSm1\nWWsdX912crVTCCE8iIS+EEJ4EAl9IYTwIBL6QgjhQST0hRDCgzjd7B2l1Ckg9QK+RQSQ6aByXIGn\n7S/IPnsK2edz00xr3aC6jZwu9C+UUirRnmlL7sLT9hdknz2F7HPNkOEdIYTwIBL6QgjhQdwx9D82\nu4Ba5mn7C7LPnkL2uQa43Zi+EEKIs3PHM30hhBBn4ZKhr5QaqpTar5RKUko9cYbH/ZVSX9ke36CU\niq39Kh3Ljn1+SCm1Rym1Qyn1q1KqmRl1OlJ1+1xlu1FKKa2UcvmZHvbss1LqBtvverdS6svartHR\n7PjbjlFKLVdKbbX9fQ8zo05HUUpNV0qdVErtOsvjSin1ju3nsUMp1d2hBWitXeoDo73zIaAF4Ads\nBzqcts3dwIe2z8cAX5lddy3s80AgyPb5XZ6wz7bt6gArMZbljDe77lr4PbcGtgJ1bbcbml13Lezz\nx8Bdts87AClm132B+9wf6A7sOsvjw4BFgAJ6Axsc+fyueKb/x0LtWuty4PeF2qsaCXxu+3wecJlt\noXZXVe0+a62Xa62LbTfXY6xS5srs+T0DPA9MAUprs7gaYs8+3w68p7XOAdBan6zlGh3Nnn3WQKjt\n8zDgWC3W53Ba65UY646czUhgpjasB8KVUk0c9fyuGPpnWqj99MXW/7RQO/D7Qu2uyp59rmoixpmC\nK6t2n20ve6O11gtqs7AaZM/vuQ3QRim1Rim1Xik1tNaqqxn27POzwHilVBqwELi3dkozzbn+fz8n\nTrEwunAcpdR4IB641OxaapJSygt4A7jF5FJqmw/GEM8AjFdzK5VSnbXWuaZWVbNuBGZorV9XSvXB\nWKWvk9baanZhrsgVz/TPZaF2Tluo3VXZtcC8Uupy4ClghNa6rJZqqynV7XMdoBOwQimVgjH2meDi\nF3Pt+T2nAQla6wqtdTJwAOMg4Krs2eeJwNcAWut1QABGjxp3Zdf/9/PliqF/IQu1u6pq91kp1Q34\nCCPwXX2cF6rZZ611ntY6Qmsdq7WOxbiOMUJr7cprbdrzt/0Dxlk+SqkIjOGew7VZpIPZs89HgMsA\nlFLtMUL/VK1WWbsSgJtss3h6A3la6+OO+uYuN7yjL2Chdldl5z6/CoQA39iuWR/RWo8wregLZOc+\nuxU793kJMEQptQeoBB7VWrvsq1g79/lhYJpS6kGMi7q3uPJJnFJqDsaBO8J2neIZwBdAa/0hxnWL\nYUASUAzc6tDnd+GfnRBCiHPkisM7QgghzpOEvhBCeBAJfSGE8CAS+kII4UEk9IUQwoNI6AshhAeR\n0BdCCA8ioS+EEB7k/wFKy+ZhWvoOLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         n_t1   n_t0  n_y1_t1  n_y1_t0   r_y1_t1   r_y1_t0    uplift\n",
            "groups                                                              \n",
            "1       408.0  446.0       68       61  0.166667  0.136771  0.029895\n",
            "2       427.0  427.0      100       38  0.234192  0.088993  0.145199\n",
            "3       438.0  416.0       79       47  0.180365  0.112981  0.067385\n",
            "4       413.0  441.0       65       47  0.157385  0.106576  0.050809\n",
            "5       433.0  421.0       74       34  0.170901  0.080760  0.090141\n",
            "6       448.0  406.0       65       46  0.145089  0.113300  0.031789\n",
            "7       409.0  445.0       26       18  0.063570  0.040449  0.023120\n",
            "8       424.0  430.0       41       40  0.096698  0.093023  0.003675\n",
            "9       444.0  410.0       49       49  0.110360  0.119512 -0.009152\n",
            "10      434.0  420.0       81       73  0.186636  0.173810  0.012826\n",
            "Qini values:  [0.008331966838155482]\n",
            "    mean: 0.008331966838155482, std: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9X42GUaj-UX",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}